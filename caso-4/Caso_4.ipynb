{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6faa8f82-8459-4b9d-b8be-b985c9f761af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Limpieza de datos con PySpark: Data Science Job Posting on Glassdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffad86de-2091-43f9-97e4-c94456fee761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Los [datos](https://tajamar365.sharepoint.com/:x:/s/3405-MasterIA2024-2025/ETYTQ0c-i6FLjM8rZ4iT1cgB6ipFAkainM-4V9M8DXsBiA?e=PeMtvh) fueron extraídos (scrapeados) del sitio web de Glassdoor y recoge los salarios de distintos puestos relacionados a Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd64de3-16db-4ce1-89ee-026c842262a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resolver los siguientes requerimientos, para cada operación/moficación imprima como van quedadndo los cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e55e35-5fe8-4a9a-a14a-d8d15dbe45b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Cargar los datos y mostrar el esquema o la informacion de las columnas y el tip de dato de cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7eeaab2-f9ff-4315-8863-67633be5c61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#vamos a probar a cargarlo tal cual esta a ver que tenemos (este es el primer csv)\n",
    "df1 = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/carlosalonsomingo@gmail.com/ds_jobs.csv\")\n",
    "\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2e503b-ebd2-4bf2-b27e-8411a1168504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Vamos a probar el segundo(pasado por el canal de big data)\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/carlosalonsomingo@gmail.com/DS_jobs_in_.csv\")\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea232a49-dc12-4550-a1bd-d8fc2d62b04a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#En vista de que son una locura, vamos a itentar que chatgpt lo normalice un poco\n",
    "df3 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/carlosalonsomingo@gmail.com/ds_jobs_cleaned.csv\")\n",
    "df3.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d8a1823-e870-4920-b9fc-14172c66046b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En vista que no tienen ningun sentido, he buscado patrones en el csv con chatgpt para poder darle algo de sentido a la informacion\n",
    "[conversación](https://chatgpt.com/share/67488911-d060-8001-8884-558e5a94c4a4)\n",
    "\n",
    "\n",
    "Aun asi , vemos que tampoco tienemuvho sentido, probablemente , es scrap no tenga demasiada logica de gestion y por ello no tenga mucho sentido, pero vmos a seguir probando(no lo usaré)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16499fa5-0f27-4557-bb0e-e14c093bac44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#vamos a probar el nuevo csv\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/carlosalonsomingo@gmail.com/ds_jobs_full_cleaned.csv\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d995965f-daba-415f-8047-5daefbd90b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vamos a probar con un codigo siguiendo el analizis de patrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9c2a3e-b69b-4237-ae49-eab094e713d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def normalize_csv_pandas_spark(input_path):\n",
    "    # Leer archivo CSV en Pandas\n",
    "    data = pd.read_csv(input_path, delimiter=';', encoding='latin1')\n",
    "\n",
    "    # Limpieza básica de la columna 'Job Description'\n",
    "    data['Job Description'] = (\n",
    "        data['Job Description']\n",
    "        .astype(str)\n",
    "        .str.replace('', '-', regex=False)  # Reemplazar caracteres especiales\n",
    "        .str.replace('\\n', ' ', regex=False)  # Eliminar saltos de línea\n",
    "        .str.replace(' +', ' ', regex=True)  # Normalizar espacios múltiples\n",
    "    )\n",
    "    \n",
    "    # Extraer \"Duties\" de las descripciones\n",
    "    data['Duties'] = data['Job Description'].str.extract(r'(Duties & Responsibilities:.*?Minimum Qualifications:)')[0]\n",
    "    data['Duties'] = data['Duties'].str.replace(r'Minimum Qualifications:', '', regex=False).str.strip()\n",
    "\n",
    "    # Eliminar secciones irrelevantes de la descripción\n",
    "    data['Job Description'] = data['Job Description'].str.replace(r'Duties & Responsibilities:.*?Minimum Qualifications:', '', regex=True)\n",
    "\n",
    "    # Limpieza básica de 'Company Name'\n",
    "    # Separar nombre de la empresa del rating si están juntos (e.g., \"CompanyName\\n4.5\")\n",
    "    data[['Company Name', 'Rating']] = data['Company Name'].str.extract(r'^(.*?)(?:\\n([\\d\\.]+))?$')\n",
    "    data['Rating'] = data['Rating'].astype(float)  # Convertir Rating a número\n",
    "\n",
    "    # Limpieza básica de 'Salary Estimate'\n",
    "    data['Salary Estimate'] = (\n",
    "        data['Salary Estimate']\n",
    "        .str.replace(r'\\(Glassdoor est.\\)', '', regex=True)\n",
    "        .str.replace(r'\\(Employer est.\\)', '', regex=True)\n",
    "        .str.replace(r'\\$', '', regex=False)\n",
    "        .str.replace(r'K', '000', regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "    \n",
    "    # Limpieza de 'Headquarters'\n",
    "    data['Headquarters'] = (\n",
    "        data['Headquarters']\n",
    "        .astype(str)\n",
    "        .str.replace('\\n', ' ', regex=False)  # Eliminar saltos de línea\n",
    "        .str.replace(' +', ' ', regex=True)  # Normalizar espacios múltiples\n",
    "        .str.strip()  # Eliminar espacios iniciales y finales\n",
    "    )\n",
    "\n",
    "    # Limpieza de 'Type of ownership'\n",
    "    data['Type of ownership'] = (\n",
    "        data['Type of ownership']\n",
    "        .astype(str)\n",
    "        .str.replace('\\n', ' ', regex=False)  # Eliminar saltos de línea\n",
    "        .str.strip()  # Eliminar espacios iniciales y finales\n",
    "    )\n",
    "\n",
    "    # Limpieza de 'Competitors'\n",
    "    data['Competitors'] = (\n",
    "        data['Competitors']\n",
    "        .astype(str)\n",
    "        .str.replace('-1', 'Unknown', regex=False)  # Reemplazar valores -1 por \"Unknown\"\n",
    "        .str.replace('\\n', ', ', regex=False)  # Reemplazar saltos de línea por comas\n",
    "        .str.strip()  # Eliminar espacios iniciales y finales\n",
    "    )\n",
    "\n",
    "    # Filtrar columnas útiles\n",
    "    columns_to_keep = [\n",
    "        'Job Title', 'Company Name', 'Rating', 'Salary Estimate', \n",
    "        'Location', 'Industry', 'Sector', 'Revenue', 'Size', \n",
    "        'Founded', 'Job Description', 'Duties', 'Headquarters',\n",
    "        'Type of ownership', 'Competitors'\n",
    "    ]\n",
    "    data_cleaned = data[columns_to_keep]\n",
    "\n",
    "    # Crear una sesión de Spark\n",
    "    spark = SparkSession.builder.appName(\"Pandas to Spark\").getOrCreate()\n",
    "\n",
    "    # Convertir DataFrame de Pandas a PySpark\n",
    "    spark_df = spark.createDataFrame(data_cleaned)\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "# Uso de la función\n",
    "# Si estás en Databricks, copia el archivo al sistema local\n",
    "local_path = \"/tmp/ds_jobs.csv\"\n",
    "dbutils.fs.cp(\"dbfs:/FileStore/shared_uploads/carlosalonsomingo@gmail.com/ds_jobs.csv\", f\"file:{local_path}\")\n",
    "\n",
    "# Procesar el archivo con Pandas y convertirlo a PySpark\n",
    "df = normalize_csv_pandas_spark(local_path)\n",
    "\n",
    "# Mostrar los resultados\n",
    "df.printSchema()  # Mostrar esquema del DataFrame\n",
    "df.display()  # Mostrar tabla en Databricks\n",
    "print(df.count())  # Mostrar el número total de filas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa7ec920-8e9c-47a0-a320-66d87dc9d212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Parece que este ultimo ya tiene mas sentido, usareos este"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a4b636b-5e44-4afa-9d44-0666b991cec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b8cf72-287a-415d-aead-7bfeb9a68a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.distinct()\n",
    "\n",
    "df.display() #Observamos que de 678 filas pasamos a 661, por lo que hemos eliminado 18 filas duplicadas\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9b97746-8b29-4550-9e1b-be2876c2a0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Decidir que hacer con los datos faltantes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaedda51-4a41-41cd-aba7-75f0e5a3c5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Decidir que hacer con los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf3ceea-7769-4b92-8b10-fe55affb1827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Esta celda tiene en cuenta el apartado 3 y el 4\n",
    "#primero vemos que columnas contienen nulos y cuantos\n",
    "from pyspark.sql import functions as F\n",
    "df_nulls = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "df_nulls.display()\n",
    "#observamos que hay 1 nulo en salary y 659 nulos en dutyes lo que nos dice que no va a aportar mucho esa columna(solo tiene un regitro con info, asique la quitamos)\n",
    "df = df.drop(F.col(\"Duties\"))\n",
    "# de momento como todo son strings type, reempalazaremos por no info\n",
    "\n",
    "df = df.fillna(\"No info\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb3b1ca-24ac-4206-8906-3ecee5d6c5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. ¿Cuántos registros tiene el csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9905ead2-ee00-4ba7-8c30-b5d0abff59d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"El csv tiene {df.count()} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9aac77-6fe5-4a54-b6ae-c152feee149f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Mostrar los valores únicos de `Job title` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c163ce-ebe9-442f-a67b-243505f7e09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_unique_title = df.select(\"Job Title\").distinct()\n",
    "df_unique_title.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df06a700-7dd6-44c8-877c-52819ffb73a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Remover la letra `K` de la columna `Salary Estimate` y multiplicar por 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c022aa3-11cd-4a60-a3ec-7580b5127cdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podriamos hacerlo multiplicando, pero de momento creo que nos conviene mantener las columnas como string, por lo menos hasta que tengamos que operar con ellas, asi hacemos mas sencillo este paso\n",
    "df = df.withColumn(\"Salary Estimate\",\n",
    "                    F.regexp_replace(F.col(\"Salary Estimate\"), \"K\", \"000\")\n",
    "                    )\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0f2bbcd-5c78-4d10-9118-e105f611b4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Mostrar los valores únicos del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779b775f-11cc-4805-8be7-38014b5360e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_unique_salary = df.select(\"Salary Estimate\").distinct()\n",
    "df_unique_salary.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5d10ab-fd9e-4d8b-bf7f-f8d25b9ed1af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Eliminar `(Glassdoor est.)` y `(Employer est.)` del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f92b2526-0a5e-46f4-b22f-22b92cd5a6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (df.withColumn(\"Salary Estimate\", F.regexp_replace(F.col(\"Salary Estimate\"), r\"\\(Glassdoor est.\\)\", \"\")) #Para lo de la r\"\\\" lo he buscado ya que no lo ravia, indica que estamos trabajando con texto raw(sin procesar), muy util para las regex que tienen caracteres con significado \"\n",
    "      .withColumn(\"Salary Estimate\", F.regexp_replace(F.col(\"Salary Estimate\"), \"\\(Employer est.\\)\", \"\")))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ff37c4-17a0-4eb4-b170-a131bd9b7844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Mostrar de mayor a menor los valores del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c19c856-8462-4c83-b3cf-9e9f6edd4b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#primero eliminamos el resto de cosas que no son valores\n",
    "\n",
    "df = df.withColumn(\"Salary Estimate\", F.regexp_replace(F.col(\"Salary Estimate\"), r\"\\$\", \"\"))\n",
    "df = df.withColumn(\"Salary Estimate\", F.regexp_replace(F.col(\"Salary Estimate\"), r\"K\", \"000\"))\n",
    "#Ahora separamos para obtener minimo y maximo. Ya que es un rango tomaremos la media para ordenarlos\n",
    "df_avg_salary_ordered = df.withColumn(\"avg_salary\", (F.split(F.col(\"Salary Estimate\"), \"-\")[0] + F.split(F.col(\"Salary Estimate\"), \"-\")[1]) / 2).orderBy(F.col(\"avg_salary\").desc())\n",
    "df_avg_salary_ordered.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c20074ca-ff4d-45ba-a548-369bd3bdadc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. De la columna `Job Description` quitar los saltos de linea `\\n` del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8514bda6-74a2-419a-91d4-37ff678caed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Job Description\", F.regexp_replace(F.col(\"Job Description\"), r\"\\n\", \" \"))#Asi seria la consulta, solo que no he metido en el df la descricion por que me daba muchos problemas()\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63b8b421-3881-4bae-95c0-b79a37422e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. De la columna `Rating` muestre los valores unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1711f5-3642-4088-86f3-b2356ec01c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_ratings = df.select(\"Rating\").distinct().orderBy(\"Rating\")\n",
    "unique_ratings.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd85fcdd-3514-4ad2-ab52-d1f21b825f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13. Del campo `Rating` reemplazar los `-1.0` por `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e09be8-66c7-4777-beac-bc246f93f29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Rating\", F.when(col(\"Rating\") == -1.0, 0.0).otherwise(col(\"Rating\")))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd30cd0d-46a3-4f2f-84f8-4c7e81d7d3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14. Mostrar los valores unicos y ordenar los valores del campo `Company Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2c4423-a99c-4735-a6df-9927279f179e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_company_names = df.select(\"Company Name\").distinct().orderBy(\"Company Name\")\n",
    "unique_company_names.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd2e2a45-3570-49a6-823c-882f5714a647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "15. Quitar todos los caracteres innecesarios que encuentres en el campo `Company Name`. Por ejemplo los saltos de linea `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1a6173-b6c0-4735-9b20-284adceb631a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Company Name\", regexp_replace(col(\"Company Name\"), r\"[\\n\\r]\", \"\"))  # Eliminar saltos de línea\n",
    "df = df.withColumn(\"Company Name\", regexp_replace(col(\"Company Name\"), r\" +\", \" \"))  # Eliminar espacios múltiples\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e594ec2-2d3f-429a-8cf4-7a6815567aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "16. En el campo `Location` convertir esa columna en dos: `City` y `State`. Las ciudades que tengas en `Location` asignar a la columna `City`. Lo mismo para `State`. Luego elimine la columna `Location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4748cbec-0fa5-4ea9-9bc8-f86bc1ffbe45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (df\n",
    "      .withColumn(\"City\", F.split(F.col(\"Location\"), \", \")[0])\n",
    "      .withColumn(\"State\", F.split(F.col(\"Location\"), \", \")[1])\n",
    "      .drop(F.col(\"Location\"))\n",
    ")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed408ad-803a-4209-9dae-57b7418724ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "17. Repetir la misma lógica de la pregunta 16 pero para el campo `Headquarters`. En Headquarters dejar solo la ciudad, mientras que para el estado añadirla a una columna nueva ` Headquarter State`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b188641d-1207-4299-bfce-fd4354280f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (df\n",
    "      .withColumn(\"Headquarter State\", F.split(F.col(\"Headquarters\"), \", \")[1])\n",
    "      .withColumn(\"Headquarters\", F.split(F.col(\"Headquarters\"), \", \")[0])\n",
    "      \n",
    ")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c9f37c-dbe5-4f0e-8f9b-7c4b4cfe8ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "18. Muestre los valores únicos del campo `Headquarter State` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292356bd-77a7-4305-b916-0dba5f51729d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_states = df.select(\"Headquarter State\").distinct()\n",
    "unique_states.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7895f10-d9c8-443b-a225-848031030eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "19. Mostrar valores unicos del campo `Size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63da7917-76c0-46bd-a057-6b204109d371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_sizes = df.select(\"Size\").distinct().orderBy(\"Size\")\n",
    "unique_sizes.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8f6a6b8-52b5-4e2e-9850-6dadc97f206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "20. Quitar 'employee' de los registros del campo `Size`. Elimine tambien otros caracteres basura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de16393-4e8c-426e-bfae-b94accc7b150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Limpiar el campo 'Size'\n",
    "df = df.withColumn(\n",
    "    \"Size\",\n",
    "    regexp_replace(col(\"Size\"), r\"employees\", \"\")  # Eliminar 'employees'\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Size\",\n",
    "    regexp_replace(col(\"Size\"), r\"[^0-9\\-to ]\", \"\")  # Eliminar caracteres basura (excepto números, guiones y 'to')\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Size\",\n",
    "    regexp_replace(col(\"Size\"), r\" +\", \" \")  # Normalizar espacios múltiples\n",
    ")\n",
    "\n",
    "# Mostrar los resultados\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36dfe0a0-9a25-40bc-8282-cba498d2badd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "21. Reemplazar la palabra 'to' por '-' en todos los registros del campo `Size`. Reemplazar tambien '-1' por 'Unknown'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c3a2b6-4efe-4bb8-8d10-e39b4464bfaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Size\", regexp_replace(col(\"Size\"), r\"\\bto\\b\", \"-\"))\n",
    "\n",
    "df = df.withColumn(\"Size\", regexp_replace(col(\"Size\"), r\"-1\", \"Unknown\"))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f598578e-b86d-4de7-a298-709ec19648ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "22. Mostrar el tipo de dato del campo `Type of ownership` y sus registros unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7e7e84-2341-477a-8a75-b737ada08a64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Tipo de dato de 'Type of ownership':\", df.schema[\"Type of ownership\"].dataType)\n",
    "unique_ownerships = df.select(\"Type of ownership\").distinct().orderBy(\"Type of ownership\")\n",
    "unique_ownerships.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f42ef72-f068-4b46-b1d3-6a5d59290322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "23. Cambiar '-1' por 'Unknown' en todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ca2683-cfec-4459-9d71-36869b85ee12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reemplazar '-1' por 'Unknown' en el campo 'Type of ownership'\n",
    "df = df.withColumn(\"Type of ownership\", F.regexp_replace(F.col(\"Type of ownership\"), r\"-1\", \"Unknown\"))\n",
    "\n",
    "# Mostrar los resultados únicos para validar el cambio\n",
    "df.select(\"Type of ownership\").distinct().orderBy(\"Type of ownership\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9f9087-9cce-4213-b99b-f573757dbd97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "24. Cambiar:  \n",
    "-  `Company - Public` por `Public Company`  \n",
    "-  `Company - Private` por `Private Company`  \n",
    "-  `Private Practice / Firm` por `Private Company`  \n",
    "-  `Subsidiary or Business Segment` por `Business`  \n",
    "-  `College / University` por `Education`  \n",
    "En todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f33f9c-6fe8-4a16-837c-cba4fff46031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Reemplazar valores en el campo 'Type of ownership'\n",
    "df = df.withColumn(\n",
    "    \"Type of ownership\",\n",
    "    when(col(\"Type of ownership\") == \"Company - Public\", \"Public Company\")\n",
    "    .when(col(\"Type of ownership\") == \"Company - Private\", \"Private Company\")\n",
    "    .when(col(\"Type of ownership\") == \"Private Practice / Firm\", \"Private Company\")\n",
    "    .when(col(\"Type of ownership\") == \"Subsidiary or Business Segment\", \"Business\")\n",
    "    .when(col(\"Type of ownership\") == \"College / University\", \"Education\")\n",
    "    .otherwise(col(\"Type of ownership\"))  # Mantener el resto sin cambios\n",
    ")\n",
    "\n",
    "# Mostrar los resultados únicos para validar los cambios\n",
    "df.select(\"Type of ownership\").distinct().orderBy(\"Type of ownership\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed329cf5-1daa-4dd9-9b16-d2ab23bbf486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "25. Mostrar el tipo de dato y los valores unicos del campo `Industry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9eb0d16-1867-4705-b10b-f8f96bdec5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Industry'\n",
    "print(\"Tipo de dato de 'Industry':\", df.schema[\"Industry\"].dataType)\n",
    "\n",
    "# Obtener y mostrar los valores únicos de 'Industry'\n",
    "unique_industries = df.select(\"Industry\").distinct().orderBy(\"Industry\")\n",
    "unique_industries.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06a12fb-1a33-4cfa-aadb-c01e00a9c4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "26. En el mismo campo de `Industry` reemplazar '-1' por 'Not Available' y '&' por 'and'.  Vuelva a imprimir los valores unicos en orden alfabético."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89d438d-8a2d-428d-8963-8b2b78478817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Reemplazar '-1' por 'Not Available' y '&' por 'and' en el campo 'Industry'\n",
    "df = df.withColumn(\"Industry\", regexp_replace(col(\"Industry\"), r\"-1\", \"Not Available\"))\n",
    "df = df.withColumn(\"Industry\", regexp_replace(col(\"Industry\"), r\"&\", \"and\"))\n",
    "\n",
    "# Obtener y mostrar los valores únicos ordenados alfabéticamente\n",
    "unique_industries = df.select(\"Industry\").distinct().orderBy(\"Industry\")\n",
    "unique_industries.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2490bb6-1f88-4410-a0c4-75826c48b70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "27. Para el campo `Sector`, muestre el tipo de dato y los valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "416622a7-6351-454d-a90c-eb90928c9b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Sector'\n",
    "print(\"Tipo de dato de 'Sector':\", df.schema[\"Sector\"].dataType)\n",
    "\n",
    "# Obtener y mostrar los valores únicos de 'Sector' en orden alfabético\n",
    "unique_sectors = df.select(\"Sector\").distinct().orderBy(\"Sector\")\n",
    "unique_sectors.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d92b2b-c4be-4aca-b734-3ef8d80b62cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "28. Aplica la misma lógica de la pregunta 26 pero sobre el campo `Sector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4942c29-cb1b-4f7f-b549-89e0f3127def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Reemplazar '-1' por 'Not Available' en el campo 'Sector'\n",
    "df = df.withColumn(\"Sector\", regexp_replace(col(\"Sector\"), r\"-1\", \"Not Available\"))\n",
    "\n",
    "# Obtener y mostrar los valores únicos de 'Sector' ordenados alfabéticamente\n",
    "unique_sectors = df.select(\"Sector\").distinct().orderBy(\"Sector\")\n",
    "unique_sectors.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a3770cb-694e-439f-8038-d28da4e3ac0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "29. Para el campo `Revenue`, muestre el tipo de dato y los valores únicos en orden ascedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a189e53-7ca3-4503-bace-aa255bd764c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Reemplazar '-1' por 'Not Available' en el campo 'Sector'\n",
    "df = df.withColumn(\"Sector\", regexp_replace(col(\"Sector\"), r\"-1\", \"Not Available\"))\n",
    "\n",
    "# Obtener y mostrar los valores únicos de 'Sector' ordenados alfabéticamente\n",
    "unique_sectors = df.select(\"Sector\").distinct().orderBy(\"Sector\")\n",
    "unique_sectors.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cb56eb9-3684-49cd-b20f-fd368d23b499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "30. En el campo `Revenue`, cambiar:  \n",
    "-  `-1` por `N/A`  \n",
    "-  `Unknown / Non-Applicable` por `N/A`  \n",
    "-  `Less than $1 million (USD)` por `Less than 1`\n",
    "-  Quitar `$` y `(USD)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724f9608-6fe6-4fab-9d89-0b91052fc275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Reemplazar '-1' por 'N/A'\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"-1\", \"N/A\"))\n",
    "\n",
    "# Reemplazar 'Unknown / Non-Applicable' por 'N/A'\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"Unknown / Non-Applicable\", \"N/A\"))\n",
    "\n",
    "# Reemplazar 'Less than $1 million (USD)' por 'Less than 1'\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"Less than \\$1 million \\(USD\\)\", \"Less than 1\"))\n",
    "\n",
    "# Quitar '$' y '(USD)'\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"\\$\", \"\"))\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"\\(USD\\)\", \"\"))\n",
    "\n",
    "# Mostrar los valores únicos para validar los cambios\n",
    "unique_revenues = df.select(\"Revenue\").distinct().orderBy(\"Revenue\")\n",
    "unique_revenues.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d89472f2-855f-4edc-bc47-d2654069bf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "31. Borrar el campo `Competitors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ac10d8a-99a6-4617-90ef-aea6cfb2baba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Modificar los valores del campo 'Revenue'\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"-1\", \"N/A\"))  # Reemplazar '-1' por 'N/A'\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"Unknown / Non-Applicable\", \"N/A\"))  # Reemplazar 'Unknown / Non-Applicable' por 'N/A'\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"Less than \\$1 million \\(USD\\)\", \"Less than 1\"))  # Reemplazar 'Less than $1 million (USD)' por 'Less than 1'\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"\\$\", \"\"))  # Eliminar '$'\n",
    "df = df.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"\\(USD\\)\", \"\"))  # Eliminar '(USD)'\n",
    "\n",
    "# Eliminar el campo 'Competitors'\n",
    "df = df.drop(\"Competitors\")\n",
    "\n",
    "# Mostrar los resultados únicos del campo 'Revenue' para validar los cambios\n",
    "df.select(\"Revenue\").distinct().orderBy(\"Revenue\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d3b8e8-a020-4a19-9d85-fb3f635ed316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "32. Crear tres columnas: `min_salary` (salario mínimo), `max_salary` (salario maximo) y `avg_salary` (salario promedio) a partir de los datos del campo `Salary Estimate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4d675e-6100-4283-b893-c8f3f7fd8dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col, expr\n",
    "\n",
    "# Dividir el campo 'Salary Estimate' en min_salary y max_salary\n",
    "df = df.withColumn(\"min_salary\", split(col(\"Salary Estimate\"), \"-\")[0].cast(\"int\"))\n",
    "df = df.withColumn(\"max_salary\", split(col(\"Salary Estimate\"), \"-\")[1].cast(\"int\"))\n",
    "\n",
    "# Calcular el promedio del salario\n",
    "df = df.withColumn(\"avg_salary\", ((col(\"min_salary\") + col(\"max_salary\")) / 2).cast(\"int\"))\n",
    "\n",
    "# Mostrar las nuevas columnas\n",
    "df.select(\"Salary Estimate\", \"min_salary\", \"max_salary\", \"avg_salary\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "359b377b-30ee-4c05-b7cd-f0f217a4d9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "33. Mostrar los valores unicos del campo `Founded` y el tipo de dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860648dd-4e04-48f7-bbf0-a502b14d7791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Founded'\n",
    "print(\"Tipo de dato de 'Founded':\", df.schema[\"Founded\"].dataType)\n",
    "\n",
    "# Obtener y mostrar los valores únicos de 'Founded' ordenados\n",
    "unique_founded = df.select(\"Founded\").distinct().orderBy(\"Founded\")\n",
    "unique_founded.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd2dd8d-9e02-46e5-b441-0b93d2775f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "34. Reemplazar '-1' por '2024' en todos los registros del campo `Founded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91cbeced-6da5-4807-86ea-61d31573ce9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Reemplazar '-1' por '2024' en el campo 'Founded'\n",
    "df = df.withColumn(\"Founded\", regexp_replace(col(\"Founded\"), r\"-1\", \"2024\"))\n",
    "\n",
    "# Mostrar los resultados únicos para validar el cambio\n",
    "df.select(\"Founded\").distinct().orderBy(\"Founded\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e823336d-0258-4af5-8d13-c9a358b9295f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "35. Crear una nueva columna o campo que se llame `company_age` con los datos que se deducen del campo `Founded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b16217f2-db42-45c6-98d4-201f952f8301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "# Calcular la edad de la compañía y crear la columna 'company_age'\n",
    "df = df.withColumn(\"company_age\", (2024 - col(\"Founded\").cast(\"int\")))\n",
    "\n",
    "# Manejar casos donde 'Founded' sea '2024' o no esté definido correctamente\n",
    "df = df.withColumn(\"company_age\", expr(\"CASE WHEN Founded = '2024' THEN 0 ELSE company_age END\"))\n",
    "\n",
    "# Mostrar los resultados\n",
    "df.select(\"Founded\", \"company_age\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49a8f7ed-434c-4ad1-ba31-eeecd65c117c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "36. Crear una columna o campo que se llame: `Job Type` y en cada registro debe ir Senior, Junior o NA según los datos del campo `Job Title`.  \n",
    "- Cambiar 'sr' o 'senior' o 'lead' o 'principal' por `Senior` en el campo `Job Type`. No olvidar las mayúsculas.\n",
    "- Cambiar 'jr' o 'jr.' o cualquier otra variante por `Junior`.  \n",
    "- En cualquier otro caso distinto a los anteriores añadir NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78faf2c6-bca6-4708-bb2e-df6e17947b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, lower\n",
    "\n",
    "# Crear la columna 'Job Type' basada en las palabras clave del campo 'Job Title'\n",
    "df = df.withColumn(\n",
    "    \"Job Type\",\n",
    "    when(lower(col(\"Job Title\")).rlike(r\"\\b(sr|senior|lead|principal)\\b\"), \"Senior\")\n",
    "    .when(lower(col(\"Job Title\")).rlike(r\"\\b(jr|jr\\.)\\b\"), \"Junior\")\n",
    "    .otherwise(\"NA\")\n",
    ")\n",
    "\n",
    "# Mostrar los resultados\n",
    "df.select(\"Job Title\", \"Job Type\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c70227b-c16c-4919-9aa2-9bf9514b76dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "37. Muestra los registros únicos del campo `Job Type`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a526808a-c436-4ee4-9d8a-2f0ff5094c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener y mostrar los valores únicos del campo 'Job Type'\n",
    "df.select(\"Job Type\").distinct().orderBy(\"Job Type\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd4dbad-048d-461f-bf2a-f54c557d16f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "38. Partiendo del campo `Job Description` se extraer todas o las principales skills solicitadas por las empresas, por ejemplo: Python, Spark , Big Data. Cada Skill debe ir en una nueva columna de tipo Binaria ( 0 , 1) o Booleana (True,  False) de modo que cada skill va ser una nueva columna y si esa skill es solicitada por la empresa colocar 1 sino colocar 0. Por ejemplo:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac99b58b-661b-4e66-b6de-0817d884a38b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lower #No se.... ya estoy desesperado y me falta el cerebro.... llevo 10 horas con este examen....\n",
    "\n",
    "# Lista de habilidades a extraer\n",
    "skills = [\"Python\", \"Spark\", \"Big Data\", \"SQL\", \"Machine Learning\", \"AWS\", \"TensorFlow\", \"PyTorch\", \"R\", \"Hadoop\"]\n",
    "\n",
    "# Crear una columna para cada habilidad como binaria (0 o 1)\n",
    "for skill in skills:\n",
    "    df = df.withColumn(\n",
    "        skill,\n",
    "        when(lower(col(\"Job Description\")).contains(skill.lower()), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "# Mostrar las columnas principales junto con las skills\n",
    "columns_to_display = [\"Job Title\", \"Salary Estimate\", \"Job Description\", \"Company Name\", \"Size\", \"Founded\", \"Type of ownership\", \"Industry\", \"Sector\", \"company_age\"] + skills\n",
    "df.select(*columns_to_display).show(truncate=False)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e86af1-e86e-48db-b9d8-832cee782b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Por ejemplo:  \n",
    "| Job Title         | Salary Estimate | Job Description                                 | Rating | Company Name       | Size       | Founded | Type of ownership         | Industry                       | Sector                         | Same State      | company_age | Python | Excel |\n",
    "|--------------------|-----------------|-------------------------------------------------|--------|--------------------|------------|---------|---------------------------|--------------------------------|--------------------------------|----------------|-------------|--------|-------|\n",
    "| Sr Data Scientist | 137000-171000   | Description The Senior Data Scientist is resp... | 3.1    | Healthfirst        | 1001-5000  | 1993    | Nonprofit Organization    | Insurance Carriers            | Insurance Carriers            | Same State      | 31          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Secure our Nation, Ignite your Future Join th... | 4.2    | ManTech            | 5001-10000 | 1968    | Public Company            | Research and Development      | Research and Development      | Same State      | 56          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Overview Analysis Group is one of the larges... | 3.8    | Analysis Group      | 1001-5000  | 1981    | Private Company           | Consulting                    | Consulting                    | Same State      | 43          | 1      | 1     |\n",
    "| Data Scientist    | 137000-171000   | JOB DESCRIPTION: Do you have a passion for Da... | 3.5    | INFICON            | 501-1000   | 2000    | Public Company            | Electrical and Electronic Manufacturing | Electrical and Electronic Manufacturing | Different State | 24          | 1      | 1     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5609a545-1510-4707-a0bd-a937e37675a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "39. Exportar dataset final a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41fe24a6-5fc3-4ec4-9b76-56fd2b3e36ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "40. Extraer todos los insights posibles que sean de valor o utilidad. Cree nuevas columnas, agrupar,  filtrar hacer varios plots que muestren dichos insights que sean de utilidad para una empresa o para un usuario. Elabore conclusiones con los insights encontrados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "986893c0-ff9c-403c-a1e6-5c0c287f92e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.- análisis salarial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfeb56e1-c929-49e1-949a-6e720ff6414a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Objetivo: Identificar tendencias salariales, comparar salarios entre diferentes tipos de trabajo, industrias, y tamaños de empresas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f86360-130c-42d6-a1af-8afc7c3d9bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61801a7f-addd-439a-9e33-2016126bb801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.-  Análisis de Habilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eb56613-5028-4013-9c19-2480b7b3e859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Objetivo: Identificar las habilidades más demandadas y su relación con el salario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632adecc-8ee1-4f29-b5a1-f4c3b141c4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "139368e3-89bc-4330-ac76-e298c85bee7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3.-  Análisis de Empresa y Tipo de Propiedad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0727bcb4-c51c-40ef-874f-809ed5d3c6cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Objetivo: Ver cómo el tipo de propiedad y antigüedad de la empresa afecta el salario y las oportunidades de empleo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370f5c6b-cf41-4d65-a482-43edb7dc6a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9570ffd-96ea-4be6-8279-8b7b92fe196c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4.- Distribucion de las ofertas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54d9b5d2-c528-4be1-8a07-7aaa21d71f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Objetivo: ver la distribucion de las ofertas segun el tipo de empleo , puesto, experiencia, industria, tamaño..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44bddfc4-356e-4dda-8758-540e4f14957f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8e94229-55e6-47a8-8811-b41c7189be1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5.- Extra: valoracion de las empresas segun diferentes parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00a448cf-613b-4dbd-a764-330ba3d7da4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Objetivo: asociar la valoracion a diferentes cualidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb8a972-2310-4ca4-9e0b-0e37e34b91dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e58518aa-bf70-48fd-bb4b-2cf15dc26e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Nota importante\n",
    "\n",
    "No he descrito las conclusiones, pero estaria encantado de contartelas cuando quieras Juanjo, pero no me da ya la cabeza, no puedo mas, he intentado hacer todo el examen por mi mismo, son las 19 de la tarde y ya no me da la cabeza para más, el examen quizas ha sido un poco desmedido, o igual es culpa mia por intentar hacerlo sin apenas ayuda, pero queria medir mis conocimientos de spark sin ayuda de chat-gpt, espero que lo entiendas, pero no puedo mas.\n",
    "La mayoria de cosas en las que me he ayudado de chatgpt estan anotadas y puedes comprobar en los videos que he hecho muy buen uso de el, no me ha resuelto nada directamente, y te puedo asegurar que en casa igual, me sabe mal no haberle podido dedicar tanto esfuerzo a esta ultima parte ya que era la mas divertida, pero he agotado mi energia en las demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3908d5d-4072-4033-ba1f-f5e7eab0d87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Caso_4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
